---
title: 'Counting, a Conjecture of Andrica and Tomescu, and the Central Limit Theorem'
date: 2024-11-28
permalink: /posts/2024/11/counting/
tags:
  - combinatorics
  - probability
---

Recently, my friends and I were talking about the following problem. Let $n$ be a positive integer and consider sequences of $+$ and $-$ signs of length $n$. To each sequence, we can assign the $i$ th term to $i$ and hence, get a series which sums to some integer. For example, with $n=4$, the sequence $(+--+)$ gives $1-2-3+4 =0$. In fact, for any consecutive 4 integers, this sequence gives 0: $k-(k+1)-(k+2)+(k+3)=0$.

**Question:** For a given $n$, how many sequences are there that sum to $0$?

Note that modulo 2, the series always sum to something congruent to $n(n+1)/2 \pmod{2}$. It's also easy to show that for $n\equiv 1,2 \pmod{4}$, the series always have odd sums and hence, cannot sum to 0. For $n \equiv 0,3 \pmod{4}$, there exist such sequences. Using the above, we see that if $n=4k$, then since every consecutive 4 integers can be assigned $\pm$ so that those 4 sum to 0, we get a sequence.

We can do somewhat better by taking a hint from 10-year old Gauss. Write out $1,...,2k$ in ascending order and beneath it, right $4k,...,2k+1$ in descending order. Each column has two entries that sum to $4k+1$ and there are $2k$ entries. So we just need to assign $+$ to $k$ of these columns and $-$ signs to the rest; this gives us ${2k \choose k}$ sequences which is our lower bound.

For $n=4k+3$, we can do a similar thing but we'll write out $1,...,2k$ in ascending order and beneath it $4k+3,...,2k+4$ in descending order. We leave $2k+1,2k+2,2k+3$ aside. Then these columns all add up to $4k+4$ and there are $2k$ of them. Like above, we can assign $\pm$ to get a sum to 0. Lastly, we can assign $(++-)$ or $(--+)$ to the three we put aside; these two assignments also give sums of 0 for the three. So we have a lower bound of $2{2k \choose k}$ for $n=4k+3$. Let's summarize what we know so far:

**Proposition:** For a given positive integer $n$, there exists a sequence of $+,-$ so that the associated sequence sums to 0 if and only if $n \equiv 0,3 \pmod{4}$.

Let's work a bit more generally. For a given $n \in \mathbb{N}$ and $m \in \mathbb{Z}$, we can count sequences for $n$ that sum to $m$. Let's call the count $Q(n,m)$.  Note that we can recursively calculate this: $Q(n,m)=Q(n-1,m-(n-1)) + Q(n-1,m+(n-1))$. Here is some code which runs this recursive definition (with caching), courtesy of one of my friends.

![label](/files/counting_code.png)

Here are the counts for up to $n=29$.
![label](/files/counts.png)

The counts increase rather quickly! This sequence is stored on the [OEIS](https://oeis.org/A063865).

### Asymptotic Behavior

Now, let's see if we can detect any asymptotic behavior. We'll try a probabilistic approach. If we assign each number in $1,...,n$ to be randomly positive or negative with 50/50 probability, what is the probability that the sum $S(n)$ is zero? Again more generally, let $q_n(m)$ be the probability that the sum is $m$. This is equal to the number of $m$-sum sequences divided by the total number of sequences: $Q(n,m)/2^n$. So if we know the probability, we can multiply by $2^n$ to get the number of sequences with $m$.

Now since the sign of each term is randomly chosen, independently of the other terms, we can represent each coordinate as an independent random variable. Let $X_k$ be the variable for the $k$ th term. It is either $-k$ or $k$ with equal probability and the sum $S(n)=\sum_{k=1}^n X_k$ is a sum of independent variables, with discrete probability distribution $q_n(m)$. These are not identical variables but the Lyapunov Central Limit Theorem roughly says that as long as we have finite means and the finite variances of these variables are not too far apart, the sum still converges to a normal distribution. 

To be more precise, Suppose $X_1,...,X_n$ is a sequence of independent random variables, each with finite expected value $\mu_i$ and variance $\sigma^2_i$. Define $s^2_n$ to be the sum of the variances. If for some $\delta >0$,

$\lim_{n\to \infty }\dfrac{1}{s_{n}^{2+\delta}} \sum_{i=1}^n \mathbb{E} \left[\left|X_i-\mu_i\right|^{2+\delta}\right]=0$ 

is satisfied, then the sum $\dfrac{X_i-\mu_i}{s_n}$ converges in distribution to a standard normal random variable $N(0,1)$.

In particular, according to Lyapunov CLT, our sequence of at random variables with variances \sigma_k^2=k^2  does meet criteria for convergence.

Variance is preserved under addition of independent random variables, so q_N(M), for large N, approaches a normal distribution with mean \mu=0 and variance
\sigma^2=\sum_{k=1}^N \sigma_k^2=\sum_{k=1}^N k^2 = \frac16 N(N+1)(2N+1) \approx \frac13 N^3

The probability distribution for a normal function is 
image.png
In summary, we've proved that as N->inf,  the discrete probability distribution q_N(M) (probability that random sequence sums to M) converges to f(M), with \mu=0, \sigma^2\approx \frac{1}{3}N^3. 

I'm fuzzy on what this means precisely, but what it *doesn't* mean is that q_N(M)\approx f(M) for any particular single value of M. This is clear because for any given N, the value of q_N(M) is actually zero for every other term! (i.e. it is zero when M!=\sum_{k=1}^N k \mod 2).  My intuition says what it actually means is something like \int_{.1\sigma}^{.2\sigma}f(M)dM \approx \sum_{M=.1\sigma}^{.2\sigma}q_N(M), where the interval size needs to be something like O(\sigma). So over a large enough interval, the probability converges, but not necessarily for individual values, 

So how can we find/approximate the specific value of q_N(0), our actual goal? Well here let us make an unfounded (sketchiness alert) assumption that the nonzero terms are smooth in some sense. Specifically assume  q_N(M)\approx 2 f(M), where we have included a scaling factor of 2 to account for the fact that the probabilistic mass is squeezed into only half of the available terms (or if Jason's Galton board had every other column blocked off, then the remaining columns would fil twice as high). In other words, we're approximating q_N(M) by integrating f over an interval of width 2 - which we further approximate by a rectangle (valid since for large N, f will be very flat at this scale).  

So conditional on the vague assumption above, we calculate
q_N(0)=2f(0)=\frac{2}{\sqrt{2\pi N^3/3}} e^0 = \sqrt{\frac{6}{\pi N^3}}

Finally, we multiply by 2^N to convert from *probability* of having zero sum to the *number of sequences* with zero sum, obtaining
Q(N,0) = 2^N 

q_N(0) = 2^N 

\sqrt{\frac{6}{\pi N^3}}
Experimentally this seems like the correct asymptotic behavior, in that the relative error seems to go to zero. (i.e. the multiplicative error is 1+O(1/N))


We can crosscheck this method for a similar but similar problem. Consider the problem obtained by replacing the sequence 1..N with all 1's.  Here if we apply the probability approach we get the same thing but the final variance is N instead of ~1/3 N^3, so the final answer looks like 2^N  \sqrt{\frac{2}{\pi N}}.   OTOH, in this case we can calculate the exact value: the sum is zero iff half of the terms are negative. For even N, there are {N\choose N/2} such sequences. Indeed, the wikipedia page for central binomial coefficients confirms that (after adjusting n=N/2) we indeed have {N\choose N/2}\approx 2^N  \sqrt{\frac{2}{\pi N}},  up to lower order terms.


![label](/files/counting_table.png)

As it turns out, it was a conjecture of Andrica and Tomescu (2002) that for $n \equiv 0,3 \pmod{4}$, as $n \to \infty$, the counts approach $2^n \sqrt{6/\pi n^3}$. This was proven by B. D. Sullivan (2013) in her [paper](https://arxiv.org/pdf/1210.8437).


end
