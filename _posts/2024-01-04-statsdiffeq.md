---
title: 'Existence and Uniqueness of Solutions to 1st Order ODEs and the Central Limit Theorem'
date: 2024-01-04
permalink: /posts/2024/01/statsdiffeq/
tags:
  - topology
  - differential equations
  - statistics
---

I'll be teaching a class on differential equations and another on statistics this coming Spring 2024. I've been wondering what similarities or relations there are between the two classes. One of the important theorems of differential equations is that given a 1st order differential equation $\frac{dy}{dt} = f(t,y)$ where $f$ and $\partial_y f$ are continuous and has some initial conditions, there exists a unique solution on a small neighborhood of the initial condition. One of the important theorems of statistics is the Central Limit Theorem which, in a very weak form, says: Suppose that $X_i$ are independent, identically distributed random variables with zero mean and variance $\sigma^2$. Then $\frac{1}{\sqrt{N}}\sum^N_{i=1}X_i \to \mathcal{N}(0,\sigma^2)$ as $N \to \infty$. Here, $\mathcal{N}(0,\sigma^2)$ means a normal distribution with mean $\mu = 0$ and variance $\sigma^2$. It's not so obvious but there is some similarities between these two theorems in that there are proofs for them which involve the notion of a fix point. The goal of this post is to spell this out a bit more (though not fully rigorously).

### Existence and Uniqueness Theorem for 1st Order Ordinary Differential Equations
Let's begin with differential equations. The existence and uniqueness theorem is more precisely as follows. Let $f(t,y)$ be a continuous function on $(a,b) \times (c,d)$, an open rectangle in the $ty$-plane. If $(t_0,y_0)$ is a point in this rectangle, there exists an $\epsilon > 0$ and function $y(t)$ defined for $t$ such that $\|t-t_0\|<\epsilon$ that solves the initial value problem $\frac{dy}{dt} = f(y,t)$ with initial condition $y(t_0) = y_0$. Furthermore, if $\partial_y f$ is also continuous on the rectangle and $y_1$ and $y_2$ are solutions to the above initial value problem for $t$ satisfying $\|t-t_0\|<\epsilon$, then $y_1(t) = y_2(t)$ on this domain.

So without the continuity condition on $\partial_y f$, we certainly can have nonuniqueness of solutions but often, we assume that $f$ is at least once differentiable and so we get both existence and uniqueness. One proof of this theorem has the following sketch. We note that a solution $y(t)$ to the differential equation would also satisfy the integral equation $y(t)-y_0 = \int^t_{t_0} f(s,y(s))\, ds$. We let $\phi_0(t) = y_0$ and $\phi_{k+1}(t) = y_0+\int^t_{t_0} f(s,\phi_k(s))\, ds$. These $\phi_k$ are called **Picard iterates** and can be viewed as being obtained by iteratively performing the same kind of integration operation on $\phi_0$. In other words, we have an operator $T$ on some space of differentiable functions. We emphasize that this operator depends on the function $f$.


This space of differentiable functions has a topology coming from a metric; let's denote it with $(X,d)$. One can show that the operator $T:X \to X$ we have is a contraction map; i.e. if $x,y\in X$, then the distance $d(Tx,Ty)$ is less than $d(x,y)$. Put another way, the operator always sends two elements to points that are closer to each other than they were previously. Then, as you iterate $T$ more and more, one expects $T^kx$ and $T^ky$ to converge to the same point as $k \to \infty$. One important thing to say is that it's not necessarily the case that $T$ "uniformly contracts." For example, for some $x,y$, perhaps $d(Tx,Ty) < \frac{1}{2}d(x,y)$ while for another pair, the new distance is only a quarter of the original distance. The **Banach Fixed Point Theorem** says that if we're dealing with a complete metric space $(X,d)$ and contracting map $T:X \to X$ so that there is a $q \in [0,1)$ such that $d(Tx,Ty) < qd(x,y)$ for all $x,y\in X$, then there exists a unique fixed point. That is, we have a point $z \in X$ so that $Tz = z$. The way to find this $z$ is to just begin with any point and just iteratively apply $T$ to it many, many times. When applied to our initial value problem, we find that the Picard iterates will converge to a unique fixed point in the space of functions. So we'll get a unique function which satisfies the integral equation and hence, the differential equation with intial condition.

I haven't thought too much about why we lose uniqueness if $\partial_y f$ isn't continuous but I suspect it's because we would then have to deal with a different space of functions and it might not be complete as a metric space. This does in fact happen; if we let $f(t,y) = 3y^{2/3}$, then $\partial_y f = 2y^{-1/3}$ which is not continuous. Then, the ODE $y' = 3y^{2/3}$ with $y(0) = 0$ has $y_1 \equiv 0$ and also $y_2 = t^3$ as solutions but they are evidently not equal. So the uniqueness of solutions can certainly fail when $\partial_y f$ is discontinuous on the domain of interest.

### The Central Limit Theorem

